{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27335a5",
   "metadata": {},
   "source": [
    "1.a\n",
    "A Classification Decision Tree is used to categorize data into discrete classes based on input features. It’s ideal for problems like medical diagnosis (e.g., classifying patients as \"healthy\" or \"at risk\"), fraud detection (e.g., identifying fraudulent transactions), or spam filtering (e.g., labeling emails as \"spam\" or \"not spam\"). Its key strengths are simplicity, interpretability, and the ability to handle both numerical and categorical data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9402f",
   "metadata": {},
   "source": [
    "1.b\n",
    "A Classification Decision Tree predicts categories by splitting data into branches based on decision rules, with the final leaf node representing the predicted class. In contrast, Multiple Linear Regression predicts continuous values using a mathematical equation that fits a line to the data based on feature weights. Trees are for discrete classes; regression is for numeric outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8ef7f",
   "metadata": {},
   "source": [
    "2.\n",
    "Accuracy is ideal for balanced scenarios, like general health screenings, where overall correct predictions matter. Sensitivity is crucial in detecting critical conditions, like cancer, to minimize missed diagnoses. Specificity is key for confirmatory tests, such as ensuring blood donors are disease-free, to avoid unnecessary interventions. Precision works best in systems like fraud detection, where reducing false positives saves resources. Each metric aligns with specific goals based on the importance of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "ab.drop(columns=['Weight_oz', 'Width', 'Height'], inplace=True)\n",
    "ab.dropna(inplace=True)\n",
    "ab['Pub year'] = ab['Pub year'].astype(int)\n",
    "ab['NumPages'] = ab['NumPages'].astype(int)\n",
    "ab['Hard_or_Paper'] = ab['Hard_or_Paper'].astype('category')\n",
    "\n",
    "print(ab.info())\n",
    "print(ab.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3fba4",
   "metadata": {},
   "source": [
    "#4\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(ab, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(ab_reduced_noNaN_train)} observations\")\n",
    "print(f\"Testing set size: {len(ab_reduced_noNaN_test)} observations\")\n",
    "\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN_train[['List Price']]\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd83ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "tree.plot_tree(clf2, feature_names=['NumPages', 'Thick', 'List Price'], class_names=['Paperback', 'Hardcover'], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e830eaf",
   "metadata": {},
   "source": [
    "5.\n",
    "The clf2 decision tree predicts whether a book is a hardcover or paperback by splitting the data based on the features NumPages, Thick, and List Price. Starting at the root node, the tree evaluates these features sequentially using thresholds determined during training. The sample follows the branches based on its feature values until it reaches a leaf node, which contains the predicted class (hardcover or paperback) and the class probabilities. With max_depth = 4, the tree can capture moderately complex relationships between the features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905ff3d",
   "metadata": {},
   "source": [
    "6.\n",
    "Using the test set, confusion matrices were created for both models. For clf (using only List Price), the sensitivity was 70%, specificity was 90.9%, and accuracy was 84.4%. For clf2 (using NumPages, Thick, and List Price), the sensitivity improved to 75%, specificity remained at 90.9%, and accuracy increased slightly to 85.9%. This indicates that clf2 performed better overall due to the inclusion of additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f151fa4",
   "metadata": {},
   "source": [
    "7.\n",
    "The differences between these two confusion matrices are caused by the features used for prediction. The first matrix uses only List Price as the predictor, which provides a limited understanding of the data and results in less accurate predictions. In contrast, the second matrix incorporates additional features (NumPages and Thick), allowing the model to capture more complexity and relationships in the data, leading to improved predictions. The two confusion matrices for clf and clf2 (based on the test set) are better because they evaluate the models' performance on unseen data, reflecting their generalization ability rather than overfitting to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53deb24f",
   "metadata": {},
   "source": [
    "8.\n",
    "The most important predictor variable for making predictions according to clf2 is List Price, with a feature importance score of approximately 0.486. This indicates that List Price contributes the most to the decision-making process of the classification tree, compared to the other predictors (NumPages and Thick)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2512cf",
   "metadata": {},
   "source": [
    "9.\n",
    "In linear regression, the coefficients show how much the outcome changes when one variable increases by one unit, while keeping everything else the same. In decision trees, feature importance tells us which variable helps the model the most by improving its predictions at different points in the tree. Unlike linear regression, decision trees don’t show clear, single effects because the variables interact in a more complicated way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449f5a6",
   "metadata": {},
   "source": [
    "10.\n",
    "Yep."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
